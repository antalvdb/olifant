{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antalvdb/olifant/blob/main/timbl_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq699BLeOfyz"
      },
      "source": [
        "# Memory-based language modeling with Olifant\n",
        "\n",
        "Looking for an LLM that is relatively eco-friendly? Memory-based language models rely on CPUs; no GPUs or TPUs are required. Training MBLMs is costly in terms of RAM, but not in terms of time or computing resources. Running an MBLM in autoregressive GPT-style mode also costs RAM, but still relies on CPUs and is reasonably fast as well, depending on the selected approximation of k-nearest neighbor classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxmB6txxNmjs"
      },
      "source": [
        "In this notebook we work with the `olifant` package, which installs all necessary components, such as the TiMBL engine. We also make use of the Hugging Face transformers library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQ-xeliyOBgd"
      },
      "outputs": [],
      "source": [
        "!pip install olifant\n",
        "\n",
        "import timbl\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import re\n",
        "import time\n",
        "import argparse\n",
        "import sys\n",
        "import ast\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to Google Drive if you have files there that you want to use in this Notebook"
      ],
      "metadata": {
        "id": "qdu2OCZrrlh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "PVNxhOSSho71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYUDWeGkNtLi"
      },
      "source": [
        "Or, download a sample MBLM model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRHCgzSgKymy"
      },
      "outputs": [],
      "source": [
        "!wget https://antalvandenbosch.nl/mblm/edufineweb_train_000001.tok.l4r0.igtree.ibase\n",
        "!wget https://antalvandenbosch.nl/mblm/edufineweb_train_000001.tok.l4r0.igtree.ibase.wgt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a global verbosity level and introduce a simple logging function"
      ],
      "metadata": {
        "id": "BUewjduduKRq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2Oybcp2Scos"
      },
      "outputs": [],
      "source": [
        "# Global verbosity level\n",
        "VERBOSITY = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_APEp2gxSoIJ"
      },
      "outputs": [],
      "source": [
        "def log(message, level=1):\n",
        "    \"\"\"Logs a message if the verbosity level is sufficient.\"\"\"\n",
        "    if VERBOSITY >= level:\n",
        "        print(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following two functions do the essential work. The def `generate_text_from_prompt` will call the Olifant classifier iteratively to generate an output text autoregressively based on a prompt given by the user. A text is generated up to a maximum number of words (200) or until a period is generated."
      ],
      "metadata": {
        "id": "JSVToSrrN7JF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hG1i9xASuwt"
      },
      "outputs": [],
      "source": [
        "def pad_prompt(words, max_len=4):\n",
        "    \"\"\"Pad or trim the list of words to make it exactly `max_len` words.\"\"\"\n",
        "    if words is None:\n",
        "        words = []  # Ensure words is a list\n",
        "    if len(words) < max_len:\n",
        "        words = ['_'] * (max_len - len(words)) + words\n",
        "    else:\n",
        "        words = words[-max_len:]\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08traJcDT3VC"
      },
      "outputs": [],
      "source": [
        "def generate_text_from_prompt(classifier, initial_prompt, max_words=200):\n",
        "    # Tokenize the initial prompt and convert tokens back to words\n",
        "    initial_tokens = tokenizer.tokenize(initial_prompt)\n",
        "\n",
        "    if initial_tokens is None:\n",
        "        log(\"Tokenization failed; 'initial_tokens' is None.\", level=1)\n",
        "        initial_tokens = []\n",
        "\n",
        "    # Prepare the initial prompt, padded or trimmed to 4 words\n",
        "    padded_instances = []\n",
        "\n",
        "    # Generate padded instances for next-token predictions\n",
        "    for i in range(len(initial_tokens)):\n",
        "        # Take the tokens up to the current position and pad them\n",
        "        instance = pad_prompt(initial_tokens[:i], max_len=4)\n",
        "        padded_instances.append((instance, initial_tokens[i] if i < len(initial_tokens) else '_'))\n",
        "\n",
        "    # Add instances to memory\n",
        "    # for input_instance, next_token in padded_instances:\n",
        "    #     log(f\"memorized: {input_instance} {next_token}\", level=2)\n",
        "    #     classifier.append(input_instance, next_token)\n",
        "\n",
        "    # Use the final part of the prompt for further generation\n",
        "    prompt_words = pad_prompt(initial_tokens)\n",
        "\n",
        "    # generated_tokens = prompt_words[:]  # Start with the prompt\n",
        "    generated_tokens = [] # Or, start empty\n",
        "\n",
        "    try:\n",
        "        # Loop until max words generated or a period token is found\n",
        "        for _ in range(max_words):\n",
        "            next_word = None\n",
        "\n",
        "            classlabel, distribution, distance = classifier.classify(prompt_words)\n",
        "\n",
        "            neighbors_output = classifier.bestNeighbours()\n",
        "\n",
        "            log(f\"Neighbors output: {neighbors_output}\", level=3)\n",
        "\n",
        "\n",
        "            # Add instance to instance base\n",
        "            classifier.append(prompt_words, classlabel)\n",
        "\n",
        "            log(f\"Prompt words: {prompt_words}\", level=2)\n",
        "            log(f\"Classlabel: {classlabel}\", level=2)\n",
        "            log(f\"Distribution: {distribution}\", level=3)\n",
        "            log(f\"Distance: {distance}\", level=3)\n",
        "\n",
        "            generated_tokens.append(classlabel)\n",
        "\n",
        "            # Shift prompt words and add the new word\n",
        "            prompt_words = prompt_words[1:] + [classlabel]\n",
        "\n",
        "            # Stop if a period is generated\n",
        "            if classlabel == \".\":\n",
        "                break\n",
        "\n",
        "        # Detokenize the generated tokens\n",
        "        generated_text = tokenizer.convert_tokens_to_string(generated_tokens)\n",
        "\n",
        "        # Strip off original padding characters\n",
        "        generated_text = generated_text.replace(\"_\", \"\").strip()\n",
        "\n",
        "        # Print the final generated text\n",
        "        log(f\"Generated text: {generated_text}\", level=0)\n",
        "\n",
        "    except Exception as e:\n",
        "        log(f\"Error: {e}\", level=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function simulates a command-line call, taking in four arguments:\n",
        "\n",
        "1. classifier,\n",
        "2. tokenizer,\n",
        "3. TiMBL arguments,\n",
        "4. verbosity level.\n",
        "\n",
        "When this cell is run, it might take a while to load the TiMBL classifier into memory (in Colab, check RAM in resources to see if the model fits). When prompted, just try some natural language text as input. When done, type 'exit'."
      ],
      "metadata": {
        "id": "Cpg9UzJEOcaF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "D2wbIDd7S16k"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Simulate command-line arguments for notebook environment\n",
        "    # sys.argv = ['olifant', '--classifier', '/content/mblm/chatbot-instruction-prompts_tok.l16r0.igtree', '--tokenizer', 'bert-base-cased', '--timbl_args', '-a1 +D', '--verbosity', '3']\n",
        "    sys.argv = ['olifant', '--classifier', 'edufineweb_train_000001.tok.l4r0.igtree', '--tokenizer', 'gpt2', '--timbl_args', '-a1 +D', '--verbosity', '0']\n",
        "\n",
        "    # Parse command-line arguments\n",
        "    parser = argparse.ArgumentParser(description=\"Memory-based text generator\")\n",
        "    parser.add_argument(\"--classifier\", type=str, required=True, help=\"Path to the Timbl classifier file\")\n",
        "    parser.add_argument(\"--tokenizer\", type=str, required=True, help=\"Name of the Hugging Face tokenizer\")\n",
        "    parser.add_argument(\"--timbl_args\", type=str, required=True, help=\"Timbl arguments as a single string (e.g., '-a4 +D')\")\n",
        "    parser.add_argument(\"--verbosity\", type=int, default=0, help=\"Verbosity level (0: silent, 1: basic, 2: detailed, 3: debug)\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Set global verbosity level\n",
        "    VERBOSITY = args.verbosity\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)\n",
        "\n",
        "    # Initialize the classifier\n",
        "    log(\"Loading TiMBL Classifier...\", level=1)\n",
        "    classifier = timbl.TimblClassifier(args.classifier, args.timbl_args)\n",
        "    classifier.load()\n",
        "\n",
        "    # Loop to continuously ask for input and classify\n",
        "    while True:\n",
        "        # Take input from the user\n",
        "        user_input = input(\"Please enter prompt (or type 'exit' to quit): \")\n",
        "\n",
        "        # Check if the user wants to exit\n",
        "        if user_input.lower() == 'exit':\n",
        "            log(\"Exiting.\", level=1)\n",
        "            break\n",
        "\n",
        "        # Pass the input to the classifier function\n",
        "        generate_text_from_prompt(classifier, user_input)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}