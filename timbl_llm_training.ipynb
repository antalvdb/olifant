{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFCjvc6ylb+F9/kKc4chSa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antalvdb/olifant/blob/main/timbl_llm_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training an Olifant memory-based language model\n",
        "\n",
        "Olifant offers an eco-friendly alternative to neural LLMs. Olifant models rely on CPUs; no GPUs or TPUs are required. Training Olifant is costly in terms of RAM, but not in terms of time or computing resources. This notebook exemplifies how to train an Olifant model. Olifant comes in two flavors:\n",
        "\n",
        "1.   **IB1**, k-Nearest Neighbor classification - accurate but slow and RAM-intensive;\n",
        "2.   **IGTree**, decision-tree classification based on prefix tree retrieval; fast, compact, but less accurate.\n",
        "\n",
        "This notebook assumes that you have uploaded a raw text file to Colab, or to Google Drive (moving the file to My Drive and mounting this drive). A link to a sample text file is offered below."
      ],
      "metadata": {
        "id": "LoOGSoMYmmCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing packages\n",
        "\n",
        "We start by installing the `olifant` package. Because we want to run `timbl` on the command line, we install that as well. Among the installed dependencies is `codecarbon`, which we will use to track CO2 emissions; it is recommended to log all estimated emissions when doing experiments."
      ],
      "metadata": {
        "id": "i8uoHuxEbSde"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYPzhABPmkgL"
      },
      "outputs": [],
      "source": [
        "!pip install olifant\n",
        "!apt-get install timbl\n",
        "\n",
        "import timbl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading the sample training data\n",
        "\n",
        "We load some sample training data. This file represents the first 100,000 lines of the first shard of the EduFineWeb refined web corpus by Hugging Face."
      ],
      "metadata": {
        "id": "kB0l0xFSo-zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://antalvandenbosch.nl/mblm/edufineweb_train_000001-100k.txt"
      ],
      "metadata": {
        "id": "TNbL0IZrpK_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenizing the training data\n",
        "\n",
        "We tokenize the training data with the GPT2 tokenizer, once developed by OpenAI for their GPT version 2 system, and made available via the Hugging Face platform. Once you train a model with a certain tokenizer, all future data will need to be processed with the same tokenizer."
      ],
      "metadata": {
        "id": "cP39_saEcAf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "def process_file(input_filename):\n",
        "    # Generate the output filename by adding \"tok\" before the file extension\n",
        "    base, ext = os.path.splitext(input_filename)\n",
        "    output_filename = f\"{base}_tok{ext}\"\n",
        "\n",
        "    # Read the input file\n",
        "    with open(input_filename, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(lines, columns=['text'])\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "\n",
        "    # Tokenize the text\n",
        "    df['tokens'] = df['text'].apply(lambda x: tokenizer.tokenize(x))\n",
        "\n",
        "    # Write the tokens to the output file\n",
        "    with open(output_filename, 'w') as file:\n",
        "        for tokens in df['tokens']:\n",
        "            # Join the tokens list into a single string and write to the file\n",
        "            file.write(' '.join(tokens) + '\\n')\n",
        "\n",
        "    print(f\"Processed file saved as {output_filename}\")\n",
        "\n",
        "# Specify the filename directly\n",
        "filename = \"edufineweb_train_000001-100k.txt\"  # Replace with your actual filename\n",
        "input_filename = os.path.join('/content', filename)\n",
        "\n",
        "# Process the file\n",
        "process_file(input_filename)"
      ],
      "metadata": {
        "id": "RpVphh6bcZ87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating training instances for TiMBL\n",
        "\n",
        "We then create a file with fixed-width training instances that consist of a 4-word context as features, and the next word as the class label to be predicted."
      ],
      "metadata": {
        "id": "bO5xfmXYdZ_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def generate_windowed_instances(file_path, output_file, window_size=4):\n",
        "    # Start with an empty list to accumulate tokens for each block\n",
        "    tokenized_text = []\n",
        "\n",
        "    with open(file_path, 'r') as file, open(output_file, 'w') as outfile:\n",
        "        for line in file:\n",
        "            # Strip leading/trailing whitespace from the line\n",
        "            stripped_line = line.strip()\n",
        "\n",
        "            # Check if the line is empty, indicating the end of a block\n",
        "            if not stripped_line:\n",
        "                # Process the accumulated tokens for the current block\n",
        "                if tokenized_text:\n",
        "                    # Pad the beginning of the tokenized text with underscores\n",
        "                    padded_text = [\"_\"] * window_size + tokenized_text\n",
        "\n",
        "                    # Generate and print each windowed instance for this block\n",
        "                    for i in range(window_size, len(padded_text) - 1):\n",
        "                        context = padded_text[i - window_size:i]\n",
        "                        target = padded_text[i]\n",
        "                        outfile.write(f\"{' '.join(context)} {target}\\n\")\n",
        "\n",
        "                        # Reset tokenized_text for the next block\n",
        "                        tokenized_text = []\n",
        "\n",
        "            else:\n",
        "                # Append tokens from the non-empty line to the current block\n",
        "                tokenized_text.extend(stripped_line.split())\n",
        "\n",
        "        # Process any remaining tokens after the last line\n",
        "        if tokenized_text:\n",
        "            padded_text = [\"_\"] * window_size + tokenized_text\n",
        "            for i in range(window_size, len(padded_text) - 1):\n",
        "                context = padded_text[i - window_size:i]\n",
        "                target = padded_text[i]\n",
        "                outfile.write(f\"{' '.join(context)} {target}\\n\")\n",
        "\n",
        "# Specify the input and output filenames directly\n",
        "input_filename = \"edufineweb_train_000001-100k_tok.txt\"  # Replace with your actual input filename\n",
        "output_filename = input_filename.replace(\".txt\", \".l4r0\")  # Generate output filename\n",
        "\n",
        "input_file_path = os.path.join('/content', input_filename)\n",
        "output_file_path = os.path.join('/content', output_filename)\n",
        "\n",
        "# Call the function to generate windowed instances and write to the output file\n",
        "generate_windowed_instances(input_file_path, output_file_path)"
      ],
      "metadata": {
        "id": "sT1pe4Pfduo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training\n",
        "\n",
        "Now we train our MBLM model with TiMBL. This can take a while and may consume high amounts of RAM.\n",
        "\n",
        "The end result is `edufineweb_train_000001-100k_tok.l4r0.ibase`, an indexed and compressed instance base suitable for TiMBL classification. In LLM terms, this is the model file that you will need for your favorite LLM inference steps.\n",
        "\n",
        "Again, TiMBL allows for two flavors:\n",
        "\n",
        "1. The option `-a0` means that the training set is compressed losslessly, with\n",
        "compression rates around 10-30%. This is the setting that implements **IB1**, k-Nearest Neighbor classification.\n",
        "\n",
        "2. With `-a1`, a strong lossy compression is applied, yielding higher compression levels around 90-95%, and considerably faster but less accurate inference. This is TiMBL's **IGTree** option.\n",
        "\n",
        "In this example, TiMBL is called from the Notebook shell commandline (it can also be called from the Python-timbl bindings). It is wrapped inside a codecarbon CO2 emission tracker. TiMBL's quite verbose output is mixed with the equally verbose codecarbon information. TiMBL goes through three phases:\n",
        "\n",
        "1. Examining: reading all instances once into memory;\n",
        "2. Indexing: building an index on all feature values and class labels;\n",
        "3. Learning: storing all instances into a decision tree (lossless with **IB1**, lossy with **IGTree**).\n",
        "\n"
      ],
      "metadata": {
        "id": "FhI__WMHfGIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from codecarbon import track_emissions\n",
        "\n",
        "@track_emissions(project_name=\"mblm-edufineweb_train_000001-100k_tok.l4r0\")\n",
        "def train_model():\n",
        "    !timbl -f edufineweb_train_000001-100k_tok.l4r0 -a0 +D -I edufineweb_train_000001-100k_tok.l4r0.ibase\n",
        "\n",
        "train_model()"
      ],
      "metadata": {
        "id": "z1zMktq4fLB6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}